#ifndef SKETCH_BB_MINHASH_H__
#define SKETCH_BB_MINHASH_H__
#include "common.h"
#include "hll.h"
#if !NDEBUG
#include <cstdarg>
#endif


namespace sketch {

#ifndef LOG_DEBUG
#    define UNDEF_LDB
#    if !NDEBUG
#        define LOG_DEBUG(...) log_debug(__PRETTY_FUNCTION__, __FILE__, __LINE__, ##__VA_ARGS__)
static int log_debug(const char *func, const char *filename, int line, const char *fmt, ...) {
    va_list args;
    va_start(args, fmt);
    int ret(std::fprintf(stderr, "[D:%s:%s:%d] ", func, filename, line));
    ret += std::vfprintf(stderr, fmt, args);
    va_end(args);
    return ret;
}
#    else
#        define LOG_DEBUG(...)
#    endif
#endif


namespace minhash {
using namespace common;

namespace detail {

// Based on content from BinDash https://github.com/zhaoxiaofei/bindash
static inline uint64_t twounivhash(uint64_t s, uint64_t t) {
    static constexpr uint64_t LARGE_PRIME = 9223372036854775783ull;
    // Constants generated by `python -c 'import random;print("UINT64_C(%s)" % hex(random.randint(0, 1 << 64)))'`
    return (UINT64_C(0x1e68e69958ce15c1) * (UINT64_C(0x84e09756b31589c9) * s + UINT64_C(0xd89576eb901ab7d3) * t) + UINT64_C(0x2f28f2976668b622)) % LARGE_PRIME;
}

template<typename T>
static constexpr T default_val() {
    return std::numeric_limits<T>::max();
}

template<typename Container>
inline int densifybin(Container &hashes) {
    using vtype = typename std::decay<decltype(hashes[0])>::type;
    const auto empty_val = default_val<vtype>();
    auto it = hashes.cbegin();
    auto min = *it, max = min;
    while(++it != hashes.end()) {
        auto v = *it;
        min = std::min(min, v);
        max = std::max(max, v);
    }
    if (max == empty_val) {
#if VERBOSE_AF
        LOG_DEBUG("Full sketch, no densification needed\n");
#endif
        return 0; // Full sketch
    }
    if (min == empty_val) {
#if VERBOSE_AF
        LOG_DEBUG("Can't densify empty sketch\n");
#endif
        return -1; // Empty sketch
    }
#if VERBOSE_AF
    LOG_DEBUG("Densifying\n");
#endif
    for (uint64_t i = 0; i < hashes.size(); i++) {
        uint64_t j = i, nattempts = 0;
        while(hashes[j] == empty_val)
            j = twounivhash(i, ++nattempts) % hashes.size();
        hashes[i] = hashes[j];
    }
    return 1;
}

template<typename T, typename Allocator>
static inline double harmonic_cardinality_estimate_impl(const std::vector<T, Allocator> &minvec) {
    const double num = (minvec.size() & (minvec.size() - 1)) == 0 ? std::ldexp(1., double(sizeof(T) * CHAR_BIT) - std::log2(minvec.size()))
                                                                  : double(UINT64_C(-1)) / minvec.size();
    double sum = 0.;
    for(const auto v: minvec)
        sum += double(v) / num;
    return std::pow(minvec.size(), 2) / sum;
    // TODO: this could be accelerated with pre-inverting num, std::accumulate, _mm{,256,512}_add_epi*T, and _mm512_reduce_add_epi*T
}

template<typename T, typename Allocator>
static inline double harmonic_cardinality_estimate_diffmax_impl(const std::vector<T, Allocator> &minvec, const double num) {
    double sum = 0.;
    for(const auto v: minvec)
        sum += double(v) / num;
    return std::pow(minvec.size(), 2) / sum;
    // TODO: this could be accelerated with pre-inverting num, std::accumulate, _mm{,256,512}_add_epi*T, and _mm512_reduce_add_epi*T
}

template<typename T, typename Allocator>
static inline double harmonic_cardinality_estimate(std::vector<T, Allocator> &minvec, bool densify=true) {
    if(densify) {
        if(detail::densifybin(minvec) < 0) {
            std::fprintf(stderr, "Failed to densify for harmonic cardinality\n");
            return 0.;
        }
    }
    return harmonic_cardinality_estimate_impl<T>(minvec);
}

template<typename T, typename Allocator>
static inline double harmonic_cardinality_estimate(const std::vector<T, Allocator> &minvec) {
    if(std::find(minvec.begin(), minvec.end(), detail::default_val<T>()) != minvec.end()) {
        std::vector<T> tmp = minvec; // copy
        detail::densifybin(tmp);
        return harmonic_cardinality_estimate_impl(tmp);
    } // Else don't worry about it, just do the thing.
    return harmonic_cardinality_estimate_impl(minvec);
}

INLINE void setnthbit(uint64_t *ptr, size_t index, bool val) {
    ptr[index / 64] |= uint64_t(val) << (index % 64);
}
template<typename T> INLINE void setnthbit(T *ptr, size_t index, bool val) {
    return setnthbit(reinterpret_cast<uint64_t *>(ptr), index, val);
}

uint64_t getnthbit(const uint64_t *ptr, size_t index) {
    return (ptr[index / 64] >> (index % 64)) & 1u;
}

template<typename T> INLINE T getnthbit(const T *ptr, size_t index) {
    return T(getnthbit(reinterpret_cast<const uint8_t *>(ptr), index));
}
INLINE uint64_t getnthbit(uint64_t val, size_t index) {
    return getnthbit(&val, index);
}

#if __SSE2__
INLINE auto matching_bits(const __m128i *s1, const __m128i *s2, uint16_t b) {
     __m128i match = ~(*s1++ ^ *s2++);
     while(--b)
         match &= ~(*s1++ ^ *s2++);
     return popcount(common::vatpos(match, 0)) + popcount(common::vatpos(match, 1));
}
#else
#error("Require SSE2")
#endif

#if __AVX2__
INLINE auto matching_bits(const __m256i *s1, const __m256i *s2, uint16_t b) {
    __m256i match = ~(*s1++ ^ *s2++);
    while(--b)
        match &= ~(*s1++ ^ *s2++);
    return popcnt256(match);
}
#endif


#if HAS_AVX_512
INLINE auto matching_bits(const __m512i *s1, const __m512i *s2, uint16_t b) {
    __m512i match = ~(*s1++ ^ *s2++);
    while(--b)
        match &= ~(*s1++ ^ *s2++);
#if defined(__AVX512VPOPCNTDQ__)
    return ::_mm512_popcnt_epi64(match);
#else
    return popcnt512(match);
#endif
}
#endif

} // namespace detail


struct FinalBBitMinHash;
template<typename T, typename Hasher>
class DivBBitMinHasher;
struct FinalDivBBitMinHash {
private:
    FinalDivBBitMinHash() {}
public:
    using value_type = uint64_t;
    double est_cardinality_;
    uint64_t nbuckets_;
    uint32_t b_;
    std::vector<value_type, Allocator<value_type>> core_;
    FinalDivBBitMinHash(unsigned nbuckets, unsigned b, double est): est_cardinality_(est), nbuckets_(nbuckets), b_(b),
        core_((value_type(b) * nbuckets_) / 64 + (nbuckets_ * value_type(b) % (sizeof(value_type) * CHAR_BIT) != 0))
    {
        assert(core_.size() % b == 0);
#if !NDEBUG
        std::fprintf(stderr, "Initializing finalbb with %u for b and %u for p. Number of u64s: %zu. Total nbits: %zu\n", b, nbuckets, core_.size(), core_.size() * 64);
#endif
    }
    void free() {
        decltype(core_) tmp;
        std::swap(tmp, core_);
    }
    FinalDivBBitMinHash(const std::string &path): FinalDivBBitMinHash(path.data()) {}
    FinalDivBBitMinHash(const char *path) {
        std::memset(this, 0, sizeof(*this));
        read(path);
    }
    FinalDivBBitMinHash(FinalDivBBitMinHash &&o) = default;
    FinalDivBBitMinHash(const FinalDivBBitMinHash &o) = default;
    template<typename T, typename Hasher=common::WangHash>
    FinalDivBBitMinHash(DivBBitMinHasher<T, Hasher> &&o): FinalDivBBitMinHash(std::move(o.finalize())) {
#if VERBOSE_AF
        LOG_DEBUG("est card %lf\n", est_cardinality_);
        LOG_DEBUG(stderr, "b: %u. nbuckets: %u. size of core: %zu\n", b_, nbuckets_, core_.size());
#endif
        o.free();
    }
    template<typename T, typename Hasher=common::WangHash>
    FinalDivBBitMinHash(const DivBBitMinHasher<T, Hasher> &o): FinalDivBBitMinHash(std::move(o.finalize())) {}
    void read(gzFile fp) {
        uint64_t arr[2];
        if(gzread(fp, arr, sizeof(arr)) != sizeof(arr)) throw std::runtime_error("Could not read from file.");
        b_ = arr[0];
        nbuckets_ = arr[1];
        if(gzread(fp, &est_cardinality_, sizeof(est_cardinality_)) != sizeof(est_cardinality_)) throw std::runtime_error("Could not read from file.");
        core_.resize(b_ * nbuckets_ / 64 + (b_ * nbuckets_ % 64 != 0));
        gzread(fp, core_.data(), sizeof(core_[0]) * core_.size());
    }
    void read(const char *path) {
        gzFile fp = gzopen(path, "rb");
        if(!fp) throw std::runtime_error(std::string("Could not open file at ") + path);
        read(fp);
        gzclose(fp);
    }
    void write(const std::string &path, int compression=6) const {write(path.data(), compression);}
    void write(const char *path, int compression=6) const {
        std::string mode = compression ? std::string("wb") + std::to_string(compression): std::string("wT");
        gzFile fp = gzopen(path, mode.data());
        if(!fp) throw std::runtime_error(std::string("Could not open file at ") + path);
        write(fp);
        gzclose(fp);
    }
    void write(gzFile fp) const {
        uint64_t arr[] {b_, nbuckets_};
        if(__builtin_expect(gzwrite(fp, arr, sizeof(arr)) != sizeof(arr), 0)) throw std::runtime_error("Could not write to file");
        if(__builtin_expect(gzwrite(fp, &est_cardinality_, sizeof(est_cardinality_)) != sizeof(est_cardinality_), 0)) throw std::runtime_error("Could not write to file");
        if(__builtin_expect(gzwrite(fp, core_.data(), core_.size() * sizeof(core_[0])) != ssize_t(core_.size() * sizeof(core_[0])), 0)) throw std::runtime_error("Could not write to file");
    }
    double frac_equal(const FinalDivBBitMinHash &o) const {
        return double(equal_bblocks(o)) / nbuckets_;
    }
    uint64_t nmin() const {
        return nbuckets_;
    }
    double jaccard_index(const FinalDivBBitMinHash &o) const {
        /*
         * reference: https://arxiv.org/abs/1802.03914.
        */
        const double b2pow = std::ldexp(1., -b_);
        double frac = frac_equal(o);
        frac -= b2pow;
        return std::max(0., frac / (1. - b2pow));
    }
    double containment_index(const FinalDivBBitMinHash &o) const {
        double ji = jaccard_index(o);
        double is = (est_cardinality_ + o.est_cardinality_) * ji / (1. + ji);
        return is / est_cardinality_;
    }
    uint64_t equal_bblocks(const FinalDivBBitMinHash &o) const {
        assert(o.core_.size() == core_.size());
        assert(b_ <= 64); // b_ > 64 not yet supported, though it could be done with a larger hash
        // p_ already guaranteed to be greater than 6
        auto l2szfloor = ilog2(nbuckets_);
        const value_type *p1 = core_.data(), *pe = core_.data() + b_ * (1ull << l2szfloor) / 64, *p2 = o.core_.data();
        uint64_t sum;
        switch(l2szfloor) {
            case 6: {
                auto match = ~(*p1++ ^ *p2++);
                while(p1 != pe) match &= ~(*p1++ ^ *p2++);
                sum = popcount(match);
                break;
            }
            case 7: {
                const __m128i *vp1 = reinterpret_cast<const __m128i *>(p1), *vp2 = reinterpret_cast<const __m128i *>(p2), *vpe = reinterpret_cast<const __m128i *>(pe);
                __m128i match = ~(*vp1++ ^ *vp2++);
                while(vp1 != vpe)
                    match &= ~(*vp1++ ^ *vp2++);
                sum = popcount(common::vatpos(match, 0)) + popcount(common::vatpos(match, 1));
                break;
            }
#if __AVX2__
            case 8: {
                sum = common::sum_of_u64s(detail::matching_bits(reinterpret_cast<const __m256i *>(p1), reinterpret_cast<const __m256i *>(p2), b_));
                break;
            }
#  if HAS_AVX_512
            case 9: {
                sum = common::sum_of_u64s(detail::matching_bits(reinterpret_cast<const __m512i *>(p1), reinterpret_cast<const __m512i *>(p2), b_));
                break;
            }
            default: {
                // Process each 'b' remainder block in
                const __m512i *vp1 = reinterpret_cast<const __m512i *>(p1), *vp2 = reinterpret_cast<const __m512i *>(p2), *vpe = reinterpret_cast<const __m512i *>(pe);
                auto lsum = detail::matching_bits(vp1, vp2, b_);
                for(size_t i = 1; i < (size_t(1) << (l2szfloor - 9_)); ++i) {
                    vp1 += b_;
                    vp2 += b_;
                    lsum = _mm512_add_epi64(detail::matching_bits(vp1, vp2, b_), lsum);
                }
                assert((value_type*)vp1 == &core_[core_.size()]);
                sum = common::sum_of_u64s(lsum);
                break;
            }
#    else /* has avx2 not not 512 */
            default: {
                const __m256i *vp1 = reinterpret_cast<const __m256i *>(p1), *vp2 = reinterpret_cast<const __m256i *>(p2);
#if !NDEBUG
                const __m256i *vpe = reinterpret_cast<const __m256i *>(pe);
#endif
                auto lsum = detail::matching_bits(vp1, vp2, b_);
                for(size_t i = 1; i < 1ull << (l2szfloor - 8u); ++i) {
                    vp1 += b_;
                    vp2 += b_;
                    lsum = _mm256_add_epi64(detail::matching_bits(vp1, vp2, b_), lsum);
                    assert(vp1 <= vpe);
                }
                sum = common::sum_of_u64s(lsum);
                break;
            }
#  endif /* avx512 or avx2 */
#else /* assume SSE2 */
            default: {
                // Process each 'b' remainder block in
                const __m128i *vp1 = reinterpret_cast<const __m128i *>(p1), *vp2 = reinterpret_cast<const __m128i *>(p2), *vpe = reinterpret_cast<const __m128i *>(pe);
                __m128i match = ~(*vp1++ ^ *vp2++);
                for(unsigned b = b_; --b;match &= ~(*vp1++ ^ *vp2++));
                auto lsum = common::sum_of_u64s(match);
                while((uint64_t *)vp1 + 2 <= (uint64_t *)vpe) {
                    match = ~(*vp1++ ^ *vp2++);
                    for(unsigned b = b_; --b; match &= ~(*vp1++ ^ *vp2++));
                    lsum += common::sum_of_u64s(match);
                }
                sum = lsum;
                break;
            }
#endif
        } // switch(l2szfloor)
        /* Now handle the rest of the bits.
         * For simplicity, do SSE2 until 64
         */
        const value_type *const pf = &core_[core_.size()];
        if(pe == pf) {
#if VERBOSE_AF
            std::fprintf(stderr, "No remainder, we are done\n");
#endif
            return sum; // If there is no remainder, we're done
        }
#if HAS_AVX_512
        {
            __m512i lsum = _mm256_set1_epi64(0);
            const __m512i *vp1 = reinterpret_cast<const __m512i *>(pe), *vp2 = reinterpret_cast<const __m512i *>(o.core_.data() +  b_ * (1ull << l2szfloor) / 64);
            while(vp1 + b_ <= reinterpret_cast<const __m512i *>(pf)) {
                __m512i match = ~(*vp1++ ^ *vp2++);
                for(unsigned b = b_; --b; match &= ~(*vp1++ ^ *vp2++));
#if __AVX512VPOPCNTDQ__
                lsum = _mm512_add_epi64(_mm512_popcnt_epi64(match), lsum);
#else
                lsum = _mm512_add_epi64(popcnt512(match), lsum);
#endif
            }
            sum += common::sum_of_u64s(lsum);
            p1 = reinterpret_cast<const uint64_t *>(vp1);
            p2 = reinterpret_cast<const uint64_t *>(vp2);
        }
#elif __AVX2__
        {
            const __m256i *vp1 = reinterpret_cast<const __m256i *>(pe), *vp2 = reinterpret_cast<const __m256i *>(o.core_.data() + b_ * (1ull << l2szfloor) / 64);
            __m256i lsum = _mm256_set1_epi64x(0);
            while(vp1 + b_ <= reinterpret_cast<const __m256i *>(pf)) {
                //std::fprintf(stderr, "I am %zu away from the end\n", reinterpret_cast<const __m256i *>(pf) - (vp1 + b_));
                __m256i match = ~(*vp1++ ^ *vp2++);
                for(unsigned b = b_; --b;match &= ~(*vp1++ ^ *vp2++));
                lsum = _mm256_add_epi64(lsum, popcnt256(match));
            }
            sum += common::sum_of_u64s(lsum);
            p1 = reinterpret_cast<const uint64_t *>(vp1);
            p2 = reinterpret_cast<const uint64_t *>(vp2);
        }
#else
        const __m128i *vp1 = reinterpret_cast<const __m128i *>(pe);
        const __m128i *vp2 = reinterpret_cast<const __m128i *>(o.core_.data() + b_ * (1ull << l2szfloor) / 64);
        while(vp1 + b_ <= reinterpret_cast<const __m128i *>(pf)) {
            __m128i match = ~(*vp1++ ^ *vp2++);
            for(unsigned b = b_; --b; match &= ~(*vp1++ ^ *vp2++));
            sum += common::sum_of_u64s(match); // Since there's no faster popcount for __m128i currently.
        }
        p1 = reinterpret_cast<const uint64_t *>(vp1);
        p2 = reinterpret_cast<const uint64_t *>(vp2);
#endif
        while(p1 < pf) {
            uint64_t match = ~(*p1++ * *p2++);
            for(unsigned b = b_; --b; match &= ~(*p1++ ^ *p2++));
            sum += popcount(match);
        }
        return sum;
    }
};

template<typename T, typename Allocator>
FinalDivBBitMinHash div_bbit_finalize(uint32_t b, const std::vector<T, Allocator> &core_ref, double cest=0.);




template<template<typename> typename Policy=policy::SizePow2Policy, typename CountType=uint32_t>
struct SuperMinHash {
    // Note:
    // Instead of maintaining real and integral portions of a hash in floating point,
    // we store these in 32-bit portions of a 64-bit value, keeping the index information
    // in the higher register and the fractional portion in a lower one.
    // This is reduced entropy per 64-bit value (when finalized), so we're quite justified
    // in b-bit minimizing.
    // The number of bits needed for full minimizer encoding in this scheme
    // is 32 + log2(m_).
    // Best of all, because our sketch only needs mod in CountType space,
    // our mods are 5 instructions vs 16-19 on x86 for fastmod.
    const Policy<CountType> pol_;
    using BType = typename std::make_signed<CountType>::type;
    uint64_t a_, i_;
    uint32_t m_;
    std::vector<CountType>                       p_;
    std::vector<uint64_t, Allocator<uint64_t>>   h_;
    std::vector<CountType>                       q_;
    std::vector<BType>                           b_;
    SuperMinHash(size_t arg): pol_(arg), a_(pol_.arg2vecsize(arg) - 1), i_(0), m_(pol_.arg2vecsize(arg)),
        p_(m_), h_(pol_.arg2vecsize(arg), uint64_t(-1)), q_(pol_.arg2vecsize(arg), -1), b_(pol_.arg2vecsize(arg), 0) {
        //std::fprintf(stderr, "Size allocated: %zu\n", pol_.arg2vecsize(arg));
        b_.back() = m_;
        assert(m_ <= std::numeric_limits<CountType>::max());
    }
    static constexpr uint64_t join_cmp(uint32_t i, uint32_t r) {
        return (uint64_t(i) << 32) | r;
    }
    size_t needed_bits() const {
        return std::ceil(32 + std::log2(m_));
    }
    void addh(uint64_t item) {
        PCGen gen(item);
        uint64_t j = 0;
        while(j < a_) {
            uint32_t r = gen();
            uint32_t k = pol_.mod(gen());
            if(q_[j] != i_) {
                q_[j] = i_;
                p_[j] = j;
            }
            if(q_[k] != i_) {
                q_[k] = i_;
                p_[k] = j;
            }
            std::swap(p_[k], p_[j]);
            auto crj = join_cmp(j, r);
            if(crj < h_[p_[j]]) {
                auto jprime = std::min(m_ - 1, uint32_t(h_[p_[j]] >> 32));
                h_[p_[j]] = crj;
                if(j < jprime) {
                    --b_[jprime];
                    ++b_[j];
                    while(b_[a_] == 0) --a_;
                }
            }
            ++j;
        }
        ++i_;
    }
    FinalDivBBitMinHash finalize(uint32_t b) const {
        assert(b < (32 + ilog2(h_.size())));
        const auto *ptr = &h_;
        decltype(h_) tmp;
        if(std::find(h_.begin(), h_.end(), UINT64_C(-1)) != h_.end()) {
            tmp = h_;
            detail::densifybin(tmp);
            ptr = &tmp;
        }
        double cest = detail::harmonic_cardinality_estimate_diffmax_impl(*ptr, h_.size() << 32);
        return div_bbit_finalize(b, *ptr, cest);
    }
    // TODO: pack this into Final{Div,}BBitMinHashers or FinalHashers.
    // TODO: cardinality estimates
    //       this would be harder because of the way we pack values
};



template<typename CountingType, typename=typename std::enable_if<
            std::is_arithmetic<CountingType>::value
        >::type>
struct FinalCountingBBitMinHash;



template<typename T, typename Hasher=common::WangHash>
class DivBBitMinHasher {
    std::vector<T> core_;
    uint32_t b_;
    schism::Schismatic<T> div_;
    __uint128_t M_; // Cached for fastmod64
    Hasher hf_;
public:
    using final_type = FinalDivBBitMinHash;
    template<typename... Args>
    DivBBitMinHasher(unsigned nbuckets, unsigned b, Args &&... args):
        core_(((nbuckets + sizeof(T) * CHAR_BIT - 1) / (sizeof(T) * CHAR_BIT) * sizeof(T) * CHAR_BIT), detail::default_val<T>()),
        b_(b), div_(core_.size()), hf_(std::forward<Args>(args)...)
    {
        if(nbuckets % 64) {
            std::fprintf(stderr, "Warning: rounded n buckets from %u for %u so that it's convenient for our comparison strategy.\n", nbuckets, unsigned(div_.d_));
        }
        assert(core_.size() % 64 == 0);
        if(b_ < 1 || b_ > 64) throw "a party";
    }
    void addh(T val) {val = hf_(val);add(val);}
    void clear() {
        std::fill(core_.begin(), core_.end(), detail::default_val<T>());
    }
    T nbuckets() const {return div_.d_;} // also core_.size()
    INLINE void add(T hv) {
        const T bucket = div_.mod(hv);
        const T quot = div_.div(hv);
        auto &ref = core_[bucket];
#ifdef NOT_THREADSAFE
        ref = std::min(quot, ref);
#else
        while(quot < ref)
            __sync_bool_compare_and_swap(std::addressof(ref), ref, quot);
#endif
    }
    void write(const char *fn, int compression=6) const {
        finalize().write(fn, compression);
    }
    void write(const std::string &fn, int compression=6) const {write(fn.data(), compression);}
    int densify() {
        const int rc = detail::densifybin(core_);
#if VERBOSE_AF
        switch(rc) {
            case -1: std::fprintf(stderr, "[W] Can't densify empty thing\n"); break;
            case 0: std::fprintf(stderr, "The densification, it does nothing\n"); break;
            case 1: std::fprintf(stderr, "Densifying something that needs it\n");
        }
#endif
        return rc;
    }
    double cardinality_estimate(MHCardinalityMode mode=HARMONIC_MEAN) const {
#if VERBOSE_AF
        if(mode != HARMONIC_MEAN) std::fprintf(stderr, "Warning: HARMONIC_MEAN is the only MHCardinalityMode for %s\n", __PRETTY_FUNCTION__);
#endif
        return detail::harmonic_cardinality_estimate(core_);
    }
    double cardinality_estimate(MHCardinalityMode mode=HARMONIC_MEAN) {
#if VERBOSE_AF
        if(mode != HARMONIC_MEAN) std::fprintf(stderr, "Warning: HARMONIC_MEAN is the only MHCardinalityMode for %s\n", __PRETTY_FUNCTION__);
#endif
        return detail::harmonic_cardinality_estimate(core_);
    }
    FinalDivBBitMinHash finalize(unsigned b=0) const;
};


template<typename T, typename Hasher=common::WangHash>
class BBitMinHasher {
    std::vector<T> core_;
    uint32_t b_, p_;
    Hasher hf_;
public:
    void free() {
        std::vector<T>().swap(core_);
    }
    using final_type = FinalBBitMinHash;
    template<typename... Args>
    BBitMinHasher(unsigned p, unsigned b, Args &&... args):
        core_(size_t(1) << p, detail::default_val<T>()), b_(b), p_(p), hf_(std::forward<Args>(args)...)
    {
        if(b_ + p_ > sizeof(T) * CHAR_BIT) {
            char buf[512];
            std::sprintf(buf, "[E:%s:%s:%d] Width of type (%zu) is insufficient for selected p/b parameters (%d/%d)",
                         __FILE__, __PRETTY_FUNCTION__, __LINE__, sizeof(T) * CHAR_BIT, int(b_), int(p_));
            throw std::runtime_error(buf);
        }
    }
    void read(const std::string &path) {read(path.data());}
    void read(const char *path) {
        throw NotImplementedError("NotImplemented function. This is likely an error, as you probabyl don't mean to call this.");
    }
    void addh(T val) {val = hf_(val);add(val);}
    void clear() {
        std::fill(core_.begin(), core_.end(), detail::default_val<T>());
    }
    void swap(BBitMinHasher &o) {
        std::swap_ranges((uint8_t *)this, (uint8_t *)this + sizeof(*this), (uint8_t *)std::addressof(o));
    }
    void add(T hv) {
        auto &ref = core_[hv>>(sizeof(T) * CHAR_BIT - p_)];
        hv <<= p_; hv >>= p_; // Clear top values
#ifdef NOT_THREADSAFE
        ref = std::min(ref, hv);
        assert(ref <= hv);
#else
        while(hv < ref)
            __sync_bool_compare_and_swap(std::addressof(ref), ref, hv);
#endif
    }
    void write(const char *fn, int compression=6, uint32_t b=0, MHCardinalityMode mode=HARMONIC_MEAN) const {
        finalize(b ? b: b_, mode).write(fn, compression);
    }
    void write(const std::string &fn, int compression=6, uint32_t b=0) const {write(fn.data(), compression, b);}
    void write(gzFile fp, uint32_t b=0) const {
        finalize(b?b:b_).write(fp);
    }
    int densify() {
        auto rc = detail::densifybin(core_);
#if VERBOSE_AF
        switch(rc) {
            case -1: std::fprintf(stderr, "[W] Can't densify empty thing\n"); break;
            case 0: std::fprintf(stderr, "The densification, it does nothing\n"); break;
            case 1: std::fprintf(stderr, "Densifying something that needs it\n");
        }
#endif
        return rc;
    }
    double make_alpha() const {
        auto m = 1u << p_;
        switch(1u << p_) {
            case 16: return .673;
            case 32: return .697;
            case 64: return .709;
            default: return 0.7213 / (1 + 1.079/m);
        }
    }
    double cardinality_estimate(MHCardinalityMode mode=HARMONIC_MEAN) const {
        if(std::find_if(core_.begin(), core_.end(), [](auto x) {return x != detail::default_val<T>();}) == core_.end())
            return 0.; // Empty sketch
        const double num = std::ldexp(1., sizeof(T) * CHAR_BIT - p_);
        double sum;
        std::vector<T> tmp;
        const std::vector<T> *ptr = &core_;
        if(std::find(core_.begin(), core_.end(), detail::default_val<T>()) != core_.end()) { // Copy and calculate from densified.
            tmp = core_;
            detail::densifybin(tmp);
            ptr = &tmp;
        }
        switch(mode) {
        case HARMONIC_MEAN: // Dampens outliers
            return detail::harmonic_cardinality_estimate(*const_cast<std::vector<T> *>(ptr), false);
        case ARITHMETIC_MEAN: // better? Still not great.
            return arithmean(ptr->begin(), ptr->end(), [num](auto x) {return num / x;}) * core_.size();
        case MEDIAN: {
            std::vector<double> ests(ptr->size());
            for(size_t i = 0; i < ests.size(); ++i)
                ests[i] = num / core_[i];
            std::sort(ests.begin(), ests.end());
            return (ests[ests.size() / 2] + (ests[ests.size() / 2 - 1])) * .5 * core_.size() * make_alpha();
        }
        case GEOMETRIC_MEAN: // better? Still not great.
            // This can be accelerated with vector class library's fast log routines and conversion operations
            return
#if 1
                geomean_invprod(ptr->begin(), ptr->end(), num) * core_.size() * make_alpha();
#else
            // Slower, simpler way:
            std::exp(std::accumulate(ptr->begin(), ptr->end(), 0., [num](auto x, auto y) {return x + std::log(num / y);}) / core_.size()) * core_.size();
#endif
            // pth root of product of all estimates, where p = core_.size()
            // exp(1/p * [log(num) * len(minimizers) - sum(log(x) for x in minimizers)])
            // Then, times the number of minimizers, because we've partitioned the data into that many streams.
        case HLL_METHOD: {
            std::array<uint64_t, 64> arr{0};
            auto diff = p_ - 1;
            for(const auto v: (*ptr))
                ++arr[v == detail::default_val<T>() ? 0: hll::clz(v) - diff];
            return hll::detail::ertl_ml_estimate(arr, p_, sizeof(T) * CHAR_BIT - p_, 0);
        }
        default: __builtin_unreachable(); // IMPOCEROUS
        }
        return sum;
    }
    BBitMinHasher &operator+=(const BBitMinHasher &o) {

    }
    FinalBBitMinHash finalize(uint32_t b=0, MHCardinalityMode mode=HARMONIC_MEAN) const;
};
template<typename T, typename Hasher=common::WangHash>
void swap(BBitMinHasher<T, Hasher> &a, BBitMinHasher<T, Hasher> &b) {
    a.swap(b);
}

template<typename T, typename CountingType, typename Hasher=common::WangHash>
class CountingBBitMinHasher: public BBitMinHasher<T, Hasher> {
    using super = BBitMinHasher<T, Hasher>;
    std::vector<CountingType, Allocator<CountingType>> counters_;
    // TODO: consider probabilistic counting
    // TODO: consider compact_vector
    // Not threadsafe currently
    // Consider bitpacking with p_ bits for counting
public:
    using final_type = FinalCountingBBitMinHash<CountingType>;
    template<typename... Args>
    CountingBBitMinHasher(unsigned p, unsigned b, Args &&... args): super(p, b, std::forward<Args>(args)...), counters_(1ull << p) {}
    void add(T hv) {
        auto ind = hv>>(sizeof(T) * CHAR_BIT - this->p_);
        auto &ref = this->core_[ind];
        hv <<= this->p_; hv >>= this->p_; // Clear top values. We could also shift/mask, but that requires two other constants vs 1.
        if(ref < hv)
            ref = hv, counters_[ind] = 1;
        else ref += (ref == hv);
    }
    FinalCountingBBitMinHash<CountingType> finalize(uint32_t b=0, MHCardinalityMode mode=HARMONIC_MEAN) const;
};


struct FinalBBitMinHash {
private:
    FinalBBitMinHash() {}
public:
    using value_type = uint64_t; // This may be templated someday
    double est_cardinality_;
    uint32_t b_, p_;
    std::vector<value_type, Allocator<value_type>> core_;
    FinalBBitMinHash(unsigned p, unsigned b, double est): est_cardinality_(est), b_(b), p_(p),
        core_((value_type(b) << p) >> 6)
    {
#if !NDEBUG
        std::fprintf(stderr, "Initializing finalbb with %u for b and %u for p. Number of u64s: %zu. Total nbits: %zu\n", b, p, core_.size(), core_.size() * 64);
#endif
    }
    void free() {
        decltype(core_) tmp;
        std::swap(tmp, core_);
    }
    FinalBBitMinHash(const std::string &path): FinalBBitMinHash(path.data()) {}
    FinalBBitMinHash(const char *path) {
        std::memset(this, 0, sizeof(*this));
        read(path);
    }
    FinalBBitMinHash(FinalBBitMinHash &&o) = default;
    FinalBBitMinHash(const FinalBBitMinHash &o) = default;
    template<typename T, typename Hasher=common::WangHash>
    FinalBBitMinHash(BBitMinHasher<T, Hasher> &&o): FinalBBitMinHash(std::move(o.finalize())) {
        o.free();
    }
    template<typename T, typename Hasher=common::WangHash>
    FinalBBitMinHash(const BBitMinHasher<T, Hasher> &o): FinalBBitMinHash(std::move(o.finalize())) {}
    double r() const {
        return std::ldexp(est_cardinality_, -int(sizeof(value_type) * CHAR_BIT - p_));
    }
    double ab() const {
        const auto _r = r();
        auto rm1 = 1. - _r;
        auto rm1p = std::pow(rm1, std::ldexp(1., b_) - 1);
        return _r * rm1p / (1. - (rm1p * rm1));
    }
    void read(gzFile fp) {
        uint32_t arr[2];
        if(gzread(fp, arr, sizeof(arr)) != sizeof(arr)) throw std::runtime_error("Could not read from file.");
        b_ = arr[0];
        p_ = arr[1];
        if(gzread(fp, &est_cardinality_, sizeof(est_cardinality_)) != sizeof(est_cardinality_)) throw std::runtime_error("Could not read from file.");
        core_.resize((value_type(b_) << p_) >> 6);
        gzread(fp, core_.data(), sizeof(core_[0]) * core_.size());
    }
    void read(const char *path) {
        gzFile fp = gzopen(path, "rb");
        if(!fp) throw std::runtime_error(std::string("Could not open file at ") + path);
        read(fp);
        gzclose(fp);
    }
    void write(const std::string &path, int compression=6) const {write(path.data(), compression);}
    void write(const char *path, int compression=6) const {
        std::string mode = compression ? std::string("wb") + std::to_string(compression): std::string("wT");
        gzFile fp = gzopen(path, mode.data());
        if(!fp) throw std::runtime_error(std::string("Could not open file at ") + path);
        write(fp);
        gzclose(fp);
    }
    void write(gzFile fp) const {
        uint32_t arr[] {b_, p_};
        if(__builtin_expect(gzwrite(fp, arr, sizeof(arr)) != sizeof(arr), 0)) throw std::runtime_error("Could not write to file");
        if(__builtin_expect(gzwrite(fp, &est_cardinality_, sizeof(est_cardinality_)) != sizeof(est_cardinality_), 0)) throw std::runtime_error("Could not write to file");
        if(__builtin_expect(gzwrite(fp, core_.data(), core_.size() * sizeof(core_[0])) != ssize_t(core_.size() * sizeof(core_[0])), 0)) throw std::runtime_error("Could not write to file");
    }
#if HAS_AVX_512
    template<typename Func1, typename Func2>
    uint64_t equal_bblocks_sub(const uint64_t *p1, const uint64_t *pe, const uint64_t *p2, const Func1 &f1, const Func2 &f2) const {
        using VT = __m512i;
        if(core_.size() * sizeof(core_[0]) < sizeof(__m512i)) {
            uint64_t sum = f2(*p1++, *p2++);
            while(p1 != pe)
                sum += f2(*p1++, *p2++);
            return sum;
        }
        const VT *vp1 = reinterpret_cast<const VT *>(p1);
        const VT *vpe = reinterpret_cast<const VT *>(pe);
        const VT *vp2 = reinterpret_cast<const VT *>(p2);
        auto sum = f1(*vp1++, *vp2++);
        while(vp1 != vpe) sum = _mm512_add_epi64(f1(*vp1++, *vp2++), sum);
        return sum_of_u64s(sum);
    }
#elif __AVX2__
    template<typename Func1, typename Func2>
    uint64_t equal_bblocks_sub(const uint64_t *p1, const uint64_t *pe, const uint64_t *p2, const Func1 &f1, const Func2 &f2) const {
        using VT = __m256i;
        if(core_.size() * sizeof(core_[0]) < sizeof(__m256i)) {
            uint64_t sum = f2(*p1++, *p2++);
            while(p1 != pe)
                sum += f2(*p1++, *p2++);
            return sum;
        }
        const VT *vp1 = reinterpret_cast<const VT *>(p1);
        const VT *vpe = reinterpret_cast<const VT *>(pe);
        const VT *vp2 = reinterpret_cast<const VT *>(p2);
        auto sum = f1(*vp1++, *vp2++);
        while(vp1 != vpe) sum = _mm256_add_epi64(f1(*vp1++, *vp2++), sum);
        return sum_of_u64s(sum);
    }
#else
    template<typename Func1, typename Func2>
    uint64_t equal_bblocks_sub(const uint64_t *p1, const uint64_t *pe, const uint64_t *p2, const Func1 &f1, const Func2 &f2) const {
        using VT = __m128i;
        uint64_t sum = 0;
        if(core_.size() * sizeof(core_[0]) >= sizeof(Space::VType)) {
            const VT *vp1 = reinterpret_cast<const VT *>(p1);
            const VT *vpe = reinterpret_cast<const VT *>(pe);
            const VT *vp2 = reinterpret_cast<const VT *>(p2);
            do {sum += f1(*vp1++, *vp2++);} while(vp1 != vpe);
            p1 = reinterpret_cast<const uint64_t *>(vp1), p2 = reinterpret_cast<const uint64_t *>(vp2);
        }
        while(p1 != pe)
            sum += f2(*p1++, *p2++);
        return sum;
    }
#endif
    uint64_t equal_bblocks(const FinalBBitMinHash &o) const {
        assert(o.core_.size() == core_.size());
        const value_type *p1 = core_.data(), *pe = core_.data() + core_.size(), *p2 = o.core_.data();
        assert(b_ <= 64); // b_ > 64 not yet supported, though it could be done with a larger hash
        // p_ already guaranteed to be greater than 6
        switch(p_) {
            case 6: {
                auto match = ~(*p1++ ^ *p2++);
                while(p1 != pe) match &= ~(*p1++ ^ *p2++);
                return popcount(match);
            }
            case 7: {
                const __m128i *vp1 = reinterpret_cast<const __m128i *>(p1), *vp2 = reinterpret_cast<const __m128i *>(p2), *vpe = reinterpret_cast<const __m128i *>(pe);
                __m128i match = ~(*vp1++ ^ *vp2++);
                while(vp1 != vpe)
                    match &= ~(*vp1++ ^ *vp2++);
                return popcount(common::vatpos(match, 0)) + popcount(common::vatpos(match, 1));
            }
#if __AVX2__
            case 8: return common::sum_of_u64s(detail::matching_bits(reinterpret_cast<const __m256i *>(p1), reinterpret_cast<const __m256i *>(p2), b_));
#  if HAS_AVX_512
            case 9: return common::sum_of_u64s(detail::matching_bits(reinterpret_cast<const __m512i *>(p1), reinterpret_cast<const __m512i *>(p2), b_));
            default: {
                // Process each 'b' remainder block in
                const __m512i *vp1 = reinterpret_cast<const __m512i *>(p1), *vp2 = reinterpret_cast<const __m512i *>(p2), *vpe = reinterpret_cast<const __m512i *>(pe);
                auto sum = detail::matching_bits(vp1, vp2, b_);
                for(size_t i = 1; i < (size_t(1) << (p_ - 9_)); ++i) {
                    vp1 += b_;
                    vp2 += b_;
                    sum = _mm512_add_epi64(detail::matching_bits(vp1, vp2, b_), sum);
                }
                assert((value_type*)vp1 == &core_[core_.size()]);
                return common::sum_of_u64s(sum);
            }
#    else /* has avx2 not not 512 */
            default: {
                const __m256i *vp1 = reinterpret_cast<const __m256i *>(p1), *vp2 = reinterpret_cast<const __m256i *>(p2);
#if !NDEBUG
                const __m256i *vpe = reinterpret_cast<const __m256i *>(pe);
#endif
                auto sum = detail::matching_bits(vp1, vp2, b_);
                for(size_t i = 1; i < 1ull << (p_ - 8u); ++i) {
                    vp1 += b_;
                    vp2 += b_;
                    sum = _mm256_add_epi64(detail::matching_bits(vp1, vp2, b_), sum);
                    assert(vp1 <= vpe);
                }
#if !NDEBUG
                auto fptr = (value_type*)(reinterpret_cast<const __m256i *>(p1) + (size_t(b_) << (p_ - 8u)));
                assert(fptr == (p1 + core_.size()) || !std::fprintf(stderr, "fptr: %p. optr: %p\n", fptr, p1 + core_.size()));
#endif
                return common::sum_of_u64s(sum);
            }
#  endif /* avx512 or avx2 */
#else /* assume SSE2 */
            default: {
                // Process each 'b' remainder block in
                const __m128i *vp1 = reinterpret_cast<const __m128i *>(p1), *vp2 = reinterpret_cast<const __m128i *>(p2), *vpe = reinterpret_cast<const __m128i *>(pe);
                __m128i match = ~(*vp1++ ^ *vp2++);
                for(unsigned b = b_; --b;match &= ~(*vp1++ ^ *vp2++));
                auto sum = popcount(*(const uint64_t *)&match) + popcount(((const uint64_t *)&match)[1]);
                while(vp1 != vpe) {
                    match = ~(*vp1++ ^ *vp2++);
                    for(unsigned b = b_; --b; match &= ~(*vp1++ ^ *vp2++));
                    sum += popcount(*(const uint64_t *)&match) + popcount(((const uint64_t *)&match)[1]);
                }
                return sum;
            }
#endif
        }
    }
    double frac_equal(const FinalBBitMinHash &o) const {
        return std::ldexp(equal_bblocks(o), -int(p_));
    }
    uint64_t nmin() const {
        return uint64_t(1) << p_;
    }
    double jaccard_index(const FinalBBitMinHash &o) const {
        /*
         * reference: https://arxiv.org/abs/1802.03914.
        */
        const double b2pow = std::ldexp(1., -b_);
        double frac = frac_equal(o);
        frac -= b2pow;
        return std::max(0., frac / (1. - b2pow));
    }
    double containment_index(const FinalBBitMinHash &o) const {
        double ji = jaccard_index(o);
        double is = (est_cardinality_ + o.est_cardinality_) * ji / (1. + ji);
        return is / est_cardinality_;
    }
};

INLINE double jaccard_index(const FinalBBitMinHash &a, const FinalBBitMinHash &b) {
    return a.jaccard_index(b);
}


#define DEFAULT_SET_CASE(num, type, p_) \
        default:\
            for(size_t ov = 0, ev = 1 << (p_ - num); ov != ev; ++ov) {\
                auto main_ptr = ret.core_.data() + ov * sizeof(type) / sizeof(FinalType) * b;\
                auto core_ptr = core_ref.data() + ov * (sizeof(type) * CHAR_BIT);\
                for(size_t _b = 0; _b < b; ++_b) {\
                    auto ptr = main_ptr + (_b * sizeof(type)/sizeof(FinalType));\
                    for(size_t i = 0; i < (sizeof(type) * CHAR_BIT); ++i)\
                        setnthbit(ptr, i, getnthbit(core_ptr[i], _b));\
                }\
            }\
            break
#define SET_CASE(num, type, p_) \
        case num:\
            for(size_t _b = 0; _b < b; ++_b) {\
                auto ptr = ret.core_.data() + (_b * sizeof(type)/sizeof(FinalType));\
                assert(core_ref.size() == (sizeof(type) * CHAR_BIT));\
                for(size_t i = 0; i < (sizeof(type) * CHAR_BIT); ++i)\
                    setnthbit(ptr, i, getnthbit(core_ref[i], _b));\
            }\
            break

template<typename T, typename Hasher>
FinalBBitMinHash BBitMinHasher<T, Hasher>::finalize(uint32_t b, MHCardinalityMode mode) const {
    b = b ? b: b_; // Use the b_ of BBitMinHasher if not specified; this is because we can make multiple kinds of bbit minhashes from the same hasher.
    std::vector<T> tmp;
    const std::vector<T> *ptr = &core_;
    if(std::find(core_.begin(), core_.end(), detail::default_val<T>()) != core_.end()) {
        tmp = core_;
        detail::densifybin(tmp);
        ptr = &tmp;
    }
    const std::vector<T> &core_ref = *ptr;
    double cest = detail::harmonic_cardinality_estimate_impl(core_ref);
    using detail::getnthbit;
    using detail::setnthbit;
    FinalBBitMinHash ret(p_, b, cest);
#if VERBOSE_AF
    LOG_DEBUG("size of ret vector: %zu. b_: %u, p_: %u. cest: %lf. this card: %lf\n", ret.core_.size(), b_, p_, cest, this->cardinality_estimate(HLL_METHOD));
#endif
    using FinalType = typename FinalBBitMinHash::value_type;
#if !NDEBUG
#define CASE_6_TEST\
                for(size_t i = 0; i < core_ref.size(); ++i) {\
                    for(size_t _b = 0; _b < b; ++_b) {\
                        assert(getnthbit(ret.core_.data() + _b, i) == getnthbit(core_ref[i], _b));\
                    }\
                }
#else
#define CASE_6_TEST
#endif
    // TODO: consider supporting non-power of 2 numbers of minimizers by subsetting to the first k <= (1<<p) minimizers.
    if(b_ == 64) {
        // We've already failed for the case of b_ + p_ being greater than the width of T
        std::memcpy(ret.core_.data(), core_ref.data(), sizeof(core_ref[0]) * core_ref.size());
    } else {
        if(__builtin_expect(p_ < 6, 0))
            throw std::runtime_error("BBit minhashing requires at least p = 6 for non-power of two b currently. We could reduce this requirement using 32-bit integers.");
        // Pack an SSE2 element for each 1 << (p_ - 7)
        switch(p_) {
        case 6:
                for(size_t _b = 0; _b < b; ++_b)
                    for(size_t i = 0; i < 64u; ++i)
                        ret.core_.operator[](i / (sizeof(T) * CHAR_BIT) * b + _b) |= (core_ref[i] & (FinalType(1) << _b)) << (i % (sizeof(FinalType) * CHAR_BIT));
            CASE_6_TEST
            break;
        SET_CASE(7, __m128i, p_);
#if __AVX2__
        SET_CASE(8, __m256i, p_);
#if HAS_AVX_512
        SET_CASE(9, __m512i, p_);

        DEFAULT_SET_CASE(9u, __m512i, p_);
#else
        DEFAULT_SET_CASE(8u, __m256i, p_);
#endif
#else /* no avx2 or 512 */
        DEFAULT_SET_CASE(7u, __m128i, p_);
#endif
        }
    }
    return ret;
}

template<typename T, typename Allocator>
FinalDivBBitMinHash div_bbit_finalize(uint32_t b, const std::vector<T, Allocator> &core_ref, double est_v) {
    using detail::getnthbit;
    using detail::setnthbit;
    const double cest = est_v ? est_v : detail::harmonic_cardinality_estimate(core_ref);
    FinalDivBBitMinHash ret(core_ref.size(), b, cest);
    //std::fprintf(stderr, "size of ret vector: %zu. b: %u, nbuckets(): %u. cest: %lf\n", ret.core_.size(), b, unsigned(core_ref.size()), cest);
    using FinalType = typename FinalDivBBitMinHash::value_type;
    assert(ret.core_.size() % b == 0);
    assert(core_ref.size() % 64 == 0);
    //std::fprintf(stderr, "core size: %zu being collapsed into b (%d)-bit samples in core of size %zu\n", core_ref.size(), int(b), ret.core_.size());
    // TODO: consider supporting non-power of 2 numbers of minimizers by subsetting to the first k <= (1<<p) minimizers.
    if(b == 64) {
        std::memcpy(ret.core_.data(), core_ref.data(), sizeof(core_ref[0]) * core_ref.size());
    } else {
        const auto l2szfloor = ilog2(core_ref.size());
        const auto pow2 = 1ull << l2szfloor;
        switch(l2szfloor) {
        case 6:
                for(size_t _b = 0; _b < b; ++_b)
                    for(size_t i = 0; i < 64u; ++i)
                        ret.core_.operator[](i / (sizeof(T) * CHAR_BIT) * b + _b) |= (core_ref[i] & (FinalType(1) << _b)) << (i % (sizeof(FinalType) * CHAR_BIT));
                CASE_6_TEST
            break;
        SET_CASE(7, __m128i, l2szfloor);
#if __AVX2__
        SET_CASE(8, __m256i, l2szfloor);
#  if HAS_AVX_512
        SET_CASE(9, __m512i, l2szfloor);

        DEFAULT_SET_CASE(9u, __m512i, l2szfloor);
#  else
        DEFAULT_SET_CASE(8u, __m256i, l2szfloor);
#  endif
#else /* no avx2 or 512 */
        DEFAULT_SET_CASE(7u, __m128i, l2szfloor);
#endif
        }
        if(pow2 == core_ref.size()) {
            LOG_DEBUG("All packed\n");
        } else {
            assert(is_pow2(core_ref.size() - (core_ref.size() & ((1ull << l2szfloor) - 1))));
#define LEFTOVERS(type)\
            for(size_t ind = pow2 / (sizeof(type) * CHAR_BIT);ind < core_ref.size() / (sizeof(type) * CHAR_BIT);++ind) {\
                auto main_ptr = ret.core_.data() + ind * sizeof(type) / sizeof(FinalType) * b;\
                auto core_ptr = core_ref.data() + ind * sizeof(type) * CHAR_BIT;\
                /* std::fprintf(stderr, "main ptr is %zd away from the end of the main thing\n", (ret.core_.data() + ret.core_.size()) - main_ptr); */\
                /* std::fprintf(stderr, "core ptr is %zd away from the end of the core thing\n", (core_ref.data() + core_ref.size()) - core_ptr); */\
                for(auto _b = 0u; _b < b; ++_b) {\
                    auto ptr = main_ptr + (_b * sizeof(type)/sizeof(FinalType));\
                    for(size_t i = 0u; i < sizeof(type) * CHAR_BIT; ++i) {\
                        setnthbit(ptr, i, getnthbit(core_ptr[i], _b));\
                    }\
                }\
            }\
            size_t ind = core_ref.size() / (sizeof(type) * CHAR_BIT) * (sizeof(type) * CHAR_BIT); // Get the last n
#if HAS_AVX_512
            LEFTOVERS(__m512i)
#elif __AVX2__
            LEFTOVERS(__m256i)
#else
            LEFTOVERS(__m128i)
#endif
            while(ind < core_ref.size()) {
                //std::fprintf(stderr, "Rem: %zd. Mod of rem: %zd\n", core_ref.size() - ind, (core_ref.size() - ind) % 64);
                auto core_ptr = core_ref.data() + ind;
                auto ref_ptr  = ret.core_.data() + (ind / (sizeof(FinalType) * CHAR_BIT) * b);
                //std::fprintf(stderr, "ret vore Rem: %zd. Mod of rem: %zd\n", &*ret.core_.end() - ref_ptr, (&*ret.core_.end() - ref_ptr) % 64);
                for(size_t _b = 0; _b < b; ++_b)
                    for(size_t i = 0; i < 64u; ++i)
                        setnthbit(ref_ptr, i, getnthbit(core_ptr, _b));
                ind += sizeof(FinalType) * CHAR_BIT;
            }
        }
    }
    return ret;
}

#undef DEFAULT_SET_CASE
#undef SET_CASE
template<typename T, typename Hasher>
FinalDivBBitMinHash DivBBitMinHasher<T, Hasher>::finalize(uint32_t b) const {
    b = b ? b: b_; // Use the b_ of DivBBitMinHasher if not specified; this is because we can make multiple kinds of bbit minhashes from the same hasher.
    std::vector<T> tmp;
    const std::vector<T> *ptr = &core_;
    if(std::find(core_.begin(), core_.end(), detail::default_val<T>()) != core_.end()) {
        tmp = core_;
        detail::densifybin(tmp);
        ptr = &tmp;
    }
    const std::vector<T> &core_ref = *ptr;
    return div_bbit_finalize<T>(b, core_ref);
}


template<typename CountingType, typename>
struct FinalCountingBBitMinHash: public FinalBBitMinHash {
    std::vector<CountingType, Allocator<CountingType>> counters_;
    FinalCountingBBitMinHash(FinalBBitMinHash &&tmp, const std::vector<CountingType, Allocator<CountingType>> &counts): FinalBBitMinHash(std::move(tmp)), counters_(counts) {}
    FinalCountingBBitMinHash(unsigned p, unsigned b, double est): FinalBBitMinHash(b, p, est), counters_(size_t(1) << this->p_) {}
    void write(gzFile fp) const {
        FinalBBitMinHash::write(fp);
        gzwrite(fp, counters_.data(), counters_.size() * sizeof(counters_[0]));
    }
    void read(gzFile fp) {
        FinalBBitMinHash::read(fp);
        counters_.resize(size_t(1) << this->p_);
        gzread(fp, counters_.data(), counters_.size() * sizeof(counters_[0]));
    }
    void write(const char *path, int compression=6) const {
        std::string mode = compression ? std::string("wb") + std::to_string(compression): std::string("wT");
        gzFile fp = gzopen(path, mode.data());
        if(!fp) throw std::runtime_error(std::string("Could not open file at ") + path);
        write(fp);
        gzclose(fp);
    }
    void read(const char *path) {
        gzFile fp = gzopen(path, "rb");
        if(!fp) throw std::runtime_error(std::string("Could not open file at ") + path);
        read(fp);
        gzclose(fp);
    }

    struct HistResult {
        uint64_t matched_sum_,
                 total_sum_,
                 matched_bits_,
                 total_bits_;
        double jaccard_index() const {
            return static_cast<double>(matched_bits_) / total_bits_;
        }
        double weighted_jaccard_index() const {
            return static_cast<double>(matched_sum_) / total_sum_;
        }
    };
    template<typename=typename std::enable_if<std::is_same<CountingType, uint32_t>::value>::type> // Only finished for uint32_t currently
    HistResult histogram_sums(const FinalCountingBBitMinHash &o) const {
        if(!std::is_same<CountingType, uint32_t>::value) throw NotImplementedError("histogram_sums only available for uint32_t");
        assert(o.core_.size() == core_.size() || !std::fprintf(stderr, "Mismatched sizes: %zu, %zu\n", core_.size(), o.core_.size()));
        const uint64_t *p1 = core_.data(), *pe = core_.data() + core_.size(), *p2 = o.core_.data();
        assert(b_ <= 64); // b_ > 64 not yet supported, though it could be done with a larger hash
        size_t offset = 0;
        uint64_t matched_sum = 0;
        __m128i total_sum = _mm_set1_epi32(0);
        uint64_t pc_sum = 0;
        for(size_t i = 0; i < 1u << (p_ - 7); ++i) {
            auto match = ~(*p1++ ^ *p2++);
            while(p1 != pe) match &= ~(*p1++ ^ *p2++);
            pc_sum += popcount(common::vatpos(match, 0)) + popcount(common::vatpos(match, 1));
            for(size_t i = 0; i < 32; ++i) {
                auto maskv = match & 0x3u;
#if defined(ENABLE_COMPUTED_GOTO) && !defined(__clang__)
                // Can this be masked with ~(v-1) somehow?
                void **labels[4] = {&&zero, &&one, &&two, &&three};
                goto *labels[maskv];
                zero: goto end;
                one: matched_sum += std::min(o.counters_[i*2], counters_[i*2]); goto end;
                two: matched_sum += std::min(o.counters_[i*2 + 1], counters_[i*2 + 1]); goto end;
                three: matched_sum += std::min(o.counters_[i*2 + 1], counters_[i*2 + 1]); matched_sum += std::min(o.counters_[i*2], counters_[i*2]);
                end: ;
#else
                switch(maskv) {
                    case 0: break;
                    case 1: matched_sum += std::min(o.counters_[i*2], counters_[i*2]); break;
                    case 2: matched_sum += std::min(o.counters_[i*2 + 1], counters_[i*2 + 1]); break;
                    case 3: matched_sum += std::min(o.counters_[i*2 + 1], counters_[i*2 + 1]); matched_sum += std::min(o.counters_[i*2], counters_[i*2]); break;
                }
#endif
                total_sum = _mm_add_epi32(total_sum, _mm_max_epi32(*((__m128i *)o.counters_.data() + 2 * i), *((__m128i *)counters_.data() + 2 * i)));
            }
        }

        return {matched_sum, common::sum_of_u64s(total_sum), pc_sum, core_.size() * sizeof(core_[0]) * sizeof(char)};
    }
};

template<typename T, typename CountingType, typename Hasher>
FinalCountingBBitMinHash<CountingType> CountingBBitMinHasher<T, CountingType, Hasher>::finalize(uint32_t b, MHCardinalityMode mode) const {
    auto bbm = BBitMinHasher<T, Hasher>::finalize(b, mode);
    return FinalCountingBBitMinHash<CountingType>(std::move(bbm), this->counters_);
}

} // minhash
namespace mh = minhash;
} // namespace sketch

#ifdef UNDEF_LDB
#  undef LOG_DEBUG
#  undef UNDEF_LDB
#endif

#endif /* #ifndef SKETCH_BB_MINHASH_H__*/
